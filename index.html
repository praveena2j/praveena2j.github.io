<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>R. Gnana Praveen</title>
  
  <meta name="author" content="R. Gnana Praveen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>R. Gnana Praveen</name>
              </p>
              <p>I am a post-doctoral researcher at the Computer Research Institute of Montreal (<a href="https://www.crim.ca/en/">CRIM</a>), working on audio-visual learning for person verification. I did PhD in artificial intelligence (focused on computer vision and affective computing) at <a href="https://liviamtl.ca/">LIVIA lab</a>, ETS Montreal, Canada under the supervision of <a href="https://www.etsmtl.ca/en/research/professors/egranger">Prof. Eric Granger</a> and <a href="https://www.etsmtl.ca/en/research/professors/pcardinal">Prof. Patrick Cardinal</a> in 2023. In my thesis, I have worked on developing weakly supervised learning (multiple instance learning) models for facial expression recognition in videos and novel attention models for audio-visual fusion in dimensional emotion recognition.
              </p>
              <p>
                Before my PhD, I had 5 years of industrial research experience in computer vision, working for giant companies as well as start-ups including <a href="https://research.samsung.com/sri-b">Samsung Research India</a>, <a href="https://www.synechron.com/">Synechron India</a> and <a href="https://upgradcampus.com/">upGradCampus India</a>. I also had the privilege of working with <a href="http://cds.iisc.ac.in/faculty/venky/">Prof. R. Venkatesh Babu</a> at <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> on crowd flow analysis in videos. I did my Masters at <a href="https://www.iitg.ac.in/">Indian Institute of Technology Guwahati</a> under the supervision of <a href="https://www.iitg.ac.in/engfac/k.karthik/">Prof. Kannan Karthik</a> in 2012.
              </p>
	       <p>
                I like to play rhythm instruments in my free time. I also prefer to read books and occasionally do blogging. <a href="https://praveena2j.blogspot.com/">Here</a> is the collection of my musings. 
              </p>
	      
              <p style="text-align:center">
                <a href="mailto:praveenrgp1988@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/GnanaPraveen_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=hOWAkqkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/GnanaPraveenR">Twitter</a> &nbsp/&nbsp
		<a href="https://www.researchgate.net/profile/Gnana-Rajasekhar">ResearchGate</a> &nbsp/&nbsp
                <a href="https://github.com/praveena2j">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/gnanapraveen/"> LinkedIn </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/praveen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/AJ_06303.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody>
		</tr>  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		
		 <heading>News</heading>
		    <ul style=‚Äúlist-style-type:square‚Äù>
		<li><sup>
                <font color="red"><strong>New!!</strong></font>
              </sup>Apr 2024: Our work on "Recursive Joint Cross-modal attention" has been accepted at <a href="https://affective-behavior-analysis-in-the-wild.github.io/6th/">ABAW@CVPR 2024</a> Workshop </li>
		<li><sup>
              </sup>Mar 2024: Achieved 2nd place in the valence-arousal challenge of 6th <a href="https://affective-behavior-analysis-in-the-wild.github.io/6th/">ABAW@CVPR 2024</a> competition </li>
		<li><sup>
              </sup>Mar 2024: Our research article on joint cross-attention for audio-visual fusion has been featured in March issue of <a href="data/IEEE Biometric Council Newsletter, March 2024.pdf">IEEE Biometrics Newsletter</a>.</li>
		<li><sup>
              </sup>Mar 2024: One paper accepted at <a href="https://2024.ieeeicme.org/">ICME 2024</a> (CORE A)!</li>
		<li><sup>
              </sup>Mar 2024: Two papers accepted at <a href="https://fg2024.ieee-biometrics.org/">FG 2024</a>!</li>
		<li>Oct  2023: Paper accepted at <a href="https://neurips.cc/Conferences/2023/">NeurIPS 2023</a> 3rd workshop on <a href="https://neurips2023-enlsp.github.io/">ENLSP</a></li>
		<li>Sep  2023: Serving as a reviewer for <b>WACV 2024</b></li>
		<li>Jun 2023: Serving as a reviewer for <b>ACM MM 2023</b></li>
		<li>Jun 2023: Presented our work "Recurrent Joint Attention for Audio-Visual Fusion in Regression-based Emotion Recognition" at <a href="https://2023.ieeeicassp.org/">ICASSP 2023</a></li>
		<li>May  2023: Successfully defended my Ph.D. Thesis titled "Deep Regression Models for Spatiotemporal Expression Recognition in Videos"</li>
		<li>Mar  2023: Started working as a post-doctoral researcher at Computer Research Institute of Montreal (<a href="https://www.crim.ca/en/">CRIM</a>)"</li>
		</ul>
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, affective computing, deep learning, and multimodal video understanding models. Most of my research revolves around video analytics, weakly supervised learning, facial behavior analysis, and audio-visual fusion. Selected publications of my work can be found below. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>



</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/RJCMA.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Recursive Joint Cross-Modal Attention for Multimodal Fusion in Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em>IEEE/CVF CVPRW</em>, 2024 &nbsp 
              <br>
              <p></p>
              <p>
				In this work, we introduced recursive fusion of joint cross-attention across audio, visual and text modalities for multimodal dimensional emotion recognition. We participated in the valence-arousal challenge of 6th ABAW competition and achieved second place. 
              </p>
            </td>
          </tr>	



		    

</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/DemoCR.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Cross-Attention is not always needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em>IEEE ICME</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <p></p>
              <p>
				In this work, we addressed the problem of weak complementary relationships across audio and visual modalities due to sarcastic or conflicting emotions. We address the limitations of cross-attention in handling weak complementary relationships by introducing a novel framework of Dynamic Cross-Attention. 
              </p>
            </td>
          </tr>	


		    
 </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/DCA_v4.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Dynamic Cross Attention for Audio-Visual Person Verification</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em>IEEE FG</em>, 2024 &nbsp 
              <br>
              <p></p>
              <p>
				In this work, we addressed the problem of weak complementary relationships for effective audio-visual fusion for person verification using dynamic cross-attention. 
              </p>
            </td>
          </tr>	


		    
	 </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ENLSP.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Audio-Visual Person Verification based on Recursive Fusion of Joint Cross-Attention</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em>IEEE FG</em>, 2024 &nbsp 
              <br>
              <p></p>
              <p>
				Proposed a recursive joint cross-attention model for effective audio-visual fusion for person verification using recursive attention across audio and visual modalities in the videos. 
              </p>
            </td>
          </tr>	

		    
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ENLSP.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://neurips2023-enlsp.github.io/papers/paper_58.pdf">
                <papertitle>Recursive Joint Cross-Attention for Audio-Visual Speaker Verification</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em>NeurIPS Workshop</em>, 2023 &nbsp 
              <br>
              <p></p>
              <p>
				Proposed a recursive joint cross-attention model for effective audio-visual fusion for speaker verification using recursive attention across audio and visual modalities in the videos. 
              </p>
            </td>
          </tr>	

		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ICASSP2023.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10095234">
                <papertitle>Recursive Joint Attention for Audio-Visual Fusion in Regression-based Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Patrick Cardinal, Eric Granger
              <br>
              <em>IEEE ICASSP</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/RecurrentJointAttentionwithLSTMs">Code</a>
              <p></p>
              <p>
				Proposed a recursive joint cross-attention model for effective fusion of audio and visual modalities by focusing on leveraging the intra-modal relationships using LSTMs and inter-modal relationships using recursive attention across audio and visual modalities in the video. 
              </p>
            </td>
          </tr>	
		
	  <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/TBIOM2022.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10005783">
                <papertitle>Audio-Visual Fusion for Emotion Recognition in the Valence-Arousal Space Using Joint Cross-Attention</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Patrick Cardinal, Eric Granger
              <br>
              <em>IEEE Tran. on BIOM</em>, 2023 &nbsp <a href="https://ieeexplore.ieee.org/document/10210211"> <font color="red"><strong>(Best of FG2021)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/Joint-Cross-Attention-for-Audio-Visual-Fusion">Code</a>

              <p></p>
              <p>
				Investigated the prospect of leveraging both intra and inter-modal relationships using joint cross-attentional audio-visual fusion. The robustness of the proposed model is further validated for missing audio modality along with interpretability analysis.  
              </p>
            </td>
          </tr>				
	<tr bgcolor="#ffffd0" >
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/CVPRW2022.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9856650">
                <papertitle>A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Wheidima Carneiro de Melo, Nasib Ullah, Haseeb Aslam, Osama Zeeshan, Th√©o Denorme, Marco Pedersoli, Alessandro L. Koerich, Simon Bacon, Patrick Cardinal, Eric Granger
              <br>
              <em>IEEE CVPRW</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/JointCrossAttention-for-AV-Fusion">Code</a>
              <p></p>
              <p>
				Proposed a joint cross-attention model for effective fusion of audio and visual modalities by focusing on leveraging the intra and inter-modal relationships across audio and visual modalities in the video. 
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/FG2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9667055">
                <papertitle>Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE FG</em>, 2021 &nbsp <font color="red"><strong>(Full Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/Cross-Attentional-AV-Fusion">Code</a>

              <p></p>
              <p>
				Proposed a cross-attentional model to leverage the intermodal characteristics across audio and visual modalities for effective audio-visual fusion.
              </p>
            </td>
          </tr>	

  		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/BMVC2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1087.pdf">
                <papertitle>Holistic Guidance for Occluded Person Re-Identification</papertitle>
              </a>
              <br>
              Madhu Kiran, <strong>R Gnana Praveen</strong>, Le Thanh Nguyen-Meidine, Soufiane Belharbi, Louis-Antoine Blais-Morin, Eric Granger
              <br>
              <em>BMVC</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/madhukiranets/HolisitcGuidanceOccReID2">Code</a>

              <p></p>
              <p>
				Proposed a Holistic Guidance (HG) method that relies on holistic (or non-occluded) data and its distribution in dissimilarity space to train on occluded datasets without the need of any external source.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/IVU2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S026288562100072X">
                <papertitle>Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>Image and Vision Computing </em>, 2021 [Impact Factor: 4.7]
              <br>
              <a href="https://github.com/praveena2j/WSDAOR">Code</a>

              <p></p>
              <p>
				Proposed a deep learning model for weakly-supervised Domain Adaptation with ordinal regression using coarse sequence level labels of videos. In particular, we have enforced ordinal relationship in the proposed model using gaussian distribution.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/SurveyTAC.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.09858">
                <papertitle>Weakly Supervised Learning for Facial Behavior Analysis: A Review</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE Tran. on Affective Computing (Submitted)</em>, 2021
              <br>
              <p></p>
              <p>
				In this paper, we have presented a comprehensive taxonomy of weakly supervised learning models for facial behavior analysis along with its challenges and potential research directions.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/FG2020.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9320216">
                <papertitle>Deep Weakly Supervised Domain Adaptation for Pain Localization in Videos</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE FG</em>, 2020
              <br>
		 <a href="https://arxiv.org/abs/1910.08173">arXiv</a> / <a href="">Bibtex(Citation)</a> / <a href="">Teaser Video</a> / <a href="">Video Presentation</a> / <a href="">Slides</a> / <a href="">Poster</a>   
              <p></p>
              <p>
				In this paper, we have proposed a novel framework of weakly supervised domain adaptation (WSDA) with limited sequence-level labels for pain localization in videos.
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ICIP2014.jpg' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7025470">
                <papertitle>Super-pixel based crowd flow segmentation in H.264 compressed videos</papertitle>
              </a>
              <br>
              Sovan Biswas, <strong>R Gnana Praveen</strong>, R. Venkatesh Babu
              <br>
              <em>IEEE ICIP</em>, 2014
              <br>
              <p></p>
              <p>
				In this paper, we have proposed a simple yet robust novel approach for the segmentation of high-density crowd flows based on super-pixels in H.264 compressed videos.
              </p>
            </td>
          </tr>	


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
<div style="text-align: center;">
    <div style="display: inline-block; margin-top: 40px;">
        <table class="analytics">
            <tr>
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7dYDY7kWpvzwkrCWFCR3UOk_Z86oCnbQE5N2dADb_pM&cl=ffffff&w=a"></script>
            </tr>
        </table>
    </div>
</div>




	
</body>

</html>
