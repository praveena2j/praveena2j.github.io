<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>R. Gnana Praveen</title>
  
  <meta name="author" content="R. Gnana Praveen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Gnana Praveen Rajasekhar</name>
              </p>
              <p>I am a final year doctoral student, advised by <a href="https://liviamtl.ca/pages/regular-members/granger-1/">Prof. Eric Granger</a> and <a href="https://liviamtl.ca/pages/regular-members/cardinal-1/">Prof. Patrick Cardinal</a> at <a href="https://liviamtl.ca/">LIVIA Lab, √âcole de technologie sup√©rieure (√âTS), Montreal</a>, where I work on computer vision and affective computing. In my thesis, I have worked on developing weakly supervised learning (multiple instance learning) models for facial expression recognition in videos. I have also worked on developing novel attention models for audio-visual fusion in dimensional emotion recognition.
              </p>
              <p>
                Prior to my PhD, I had 5 years of industrial research experience in computer vision, working for giant companies as well as start-ups including <a href="https://research.samsung.com/sri-b">Samsung Research India</a>, <a href="https://www.synechron.com/">Synechron India</a> and <a href="https://upgradcampus.com/">upGradCampus India</a>. I also had the privilege of working with <a href="http://cds.iisc.ac.in/faculty/venky/">Prof. R. Venkatesh Babu</a> at <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> on crowd flow analysis in videos. I did my Masters at <a href="https://www.iitg.ac.in/">Indian Institute of Technology Guwahati</a>.
              </p>
	      
              <p style="text-align:center">
                <a href="mailto:praveenrgp1988@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Gnanapraveen_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=hOWAkqkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/GnanaPraveenR">Twitter</a> &nbsp/&nbsp
		<a href="https://www.researchgate.net/profile/Gnana-Rajasekhar">ResearchGate</a> &nbsp/&nbsp
                <a href="https://github.com/praveena2j">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/gnanapraveen/"> LinkedIn </a> &nbsp/&nbsp
                <a href="http://praveena2j.blogspot.com/"> Blog </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/praveen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/AJ_06303.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody>
		</tr>  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		
		 <heading>News</heading>
		    <ul style=‚Äúlist-style-type:square‚Äù>
		<li>Sep  2023: Serving as a reviewer for WACV 2024</li>
		<li>June 2023: Serving as a reviewer for ACM MM 2023</li>
		<li>June 2023: Presented our work "Recurrent Joint Attention for Audio-Visual Fusion in Regression-based Emotion Recognition" at ICASSP 2023</li>
		<li>May  2023: Successfully defended my Ph.D. Thesis titled "Deep Regression Models for Spatiotemporal Expression Recognition in Videos"</li>

		</ul>
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, affective computing, deep learning, and multimodal video understanding models. Most of my research revolves around video analytics, weakly supervised learning, facial behaviour analysis and audio-visual fusion. Selected publications of my work can be found below.
              </p>
            </td>
		   
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ICASSP2023.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Recurrent Joint Attention for Audio-Visual Fusion in Regression-based Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Patrick Cardinal, Eric Granger
              <br>
              <em>IEEE ICASSP</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/RecurrentJointAttentionwithLSTMs">Code</a>
              <p></p>
              <p>
				Proposed a recursive joint cross-attention model for effective fusion of audio and visual modalities by focusing on leveraging the intra-modal relationships using LSTMs and inter-modal relationships using recursive attention across audio and visual modalities in the video. 
              </p>
            </td>
          </tr>	
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/TBIOM2022.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10005783">
                <papertitle>Audio-Visual Fusion for Emotion Recognition in the Valence-Arousal Space Using Joint Cross-Attention</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Patrick Cardinal, Eric Granger
              <br>
              <em>IEEE Tran. on BIOM</em>, 2023 &nbsp <font color="red"><strong>(Best of FG2021)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/Joint-Cross-Attention-for-Audio-Visual-Fusion">Code</a>

              <p></p>
              <p>
				Investigated the prospect of leveraging both intra and inter-modal relationships using joint cross-attentional audio-visual fusion. The robustness of the proposed model is further validated for missing audio modality along with interpretability analysis.  
              </p>
            </td>
          </tr>				
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/CVPRW2022.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9856650">
                <papertitle>A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Wheidima Carneiro de Melo, Nasib Ullah, Haseeb Aslam, Osama Zeeshan, Th√©o Denorme, Marco Pedersoli, Alessandro L. Koerich, Simon Bacon, Patrick Cardinal, Eric Granger
              <br>
              <em>IEEE CVPRW</em>, 2022
              <br>
              <a href="https://github.com/praveena2j/JointCrossAttention-for-AV-Fusion">Code</a>
              <p></p>
              <p>
				Proposed a joint cross-attention model for effective fusion of audio and visual modalities by focusing on leveraging the intra and inter-modal relationships across audio and visual modalities in the video. 
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/FG2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9667055">
                <papertitle>Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE FG</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/Cross-Attentional-AV-Fusion">Code</a>

              <p></p>
              <p>
				Proposed a cross attentional model to leverage the inter modal characteristics across audio and visual modalities for effective audio visual fusion.
              </p>
            </td>
          </tr>	

  		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/BMVC2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1087.pdf">
                <papertitle>Holistic Guidance for Occluded Person Re-Identification</papertitle>
              </a>
              <br>
              Madhu Kiran, <strong>R Gnana Praveen</strong>, Le Thanh Nguyen-Meidine, Soufiane Belharbi, Louis-Antoine Blais-Morin, Eric Granger
              <br>
              <em>BMVC</em>, 2021
              <br>
              <a href="https://github.com/madhukiranets/HolisitcGuidanceOccReID2">Code</a>

              <p></p>
              <p>
				Proposed a Holistic Guidance (HG) method that relies on holistic (or non-occluded) data and its distribution in dissimilarity space to train on occluded dataset without the need of any external source.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/IVU2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S026288562100072X">
                <papertitle>Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>Image and Vision Computing </em>, 2021
              <br>
              <a href="https://github.com/praveena2j/WSDAOR">Code</a>

              <p></p>
              <p>
				Proposed a deep learning model for weakly-supervised Domain Adaptation with ordinal regression using coarse sequence level labels of videos. In particular, we have enforced ordinal relationship in the proposed model using gaussian distribution.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/SurveyTAC.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.09858">
                <papertitle>Weakly Supervised Learning for Facial Behavior Analysis : A Review</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE Tran. on Affective Computing (Submitted)</em>, 2021
              <br>
              <p></p>
              <p>
				In this paper, we have presented a comprehensive taxonomy of weakly supervised learning models for facial behaviour analysis along with its challenges and potential research directions.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/FG2020.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9320216">
                <papertitle>Deep Weakly Supervised Domain Adaptation for Pain Localization in Videos</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE FG</em>, 2020
              <br>
              <p></p>
              <p>
				In this paper, we have proposed a novel framework of weakly supervised domain adaptation (WSDA) with limited sequence level labels for pain localization in videos.
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ICIP2014.jpg' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7025470">
                <papertitle>Super-pixel based crowd flow segmentation in H.264 compressed videos</papertitle>
              </a>
              <br>
              Sovan Biswas, <strong>R Gnana Praveen</strong>, R. Venkatesh Babu
              <br>
              <em>IEEE ICIP</em>, 2014
              <br>
              <p></p>
              <p>
				In this paper, we have proposed a simple yet robust novel approach for segmentation of high density crowd flows based on super-pixels in H.264 compressed videos.
              </p>
            </td>
          </tr>	


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
