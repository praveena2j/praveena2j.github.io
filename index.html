<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>R. Gnana Praveen</title>
  
  <meta name="author" content="R. Gnana Praveen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/AJ_06303.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>R. Gnana Praveen</name>
              </p>
              <p>I am a post-doctoral researcher at the Computer Research Institute of Montreal (<a href="https://www.crim.ca/en/">CRIM</a>), working on audio-visual learning for person verification and emotion recognition. I did PhD in artificial intelligence (focused on computer vision and affective computing) at <a href="https://liviamtl.ca/">LIVIA lab</a>, ETS Montreal, Canada under the supervision of <a href="https://www.etsmtl.ca/en/research/professors/egranger">Prof. Eric Granger</a> and <a href="https://www.etsmtl.ca/en/research/professors/pcardinal">Prof. Patrick Cardinal</a> in 2023. In my thesis, I have worked on developing weakly supervised learning (multiple instance learning) models for facial expression recognition in videos and novel attention models for audio-visual fusion in dimensional emotion recognition.
              </p>
              <p>
                Before my PhD, I had 5 years of industrial research experience in computer vision, working for giant companies as well as start-ups including <a href="https://research.samsung.com/sri-b">Samsung Research India</a>, <a href="https://www.synechron.com/">Synechron India</a> and <a href="https://upgradcampus.com/">upGradCampus India</a>. I also had the privilege of working with <a href="http://cds.iisc.ac.in/faculty/venky/">Prof. R. Venkatesh Babu</a> at <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> on crowd flow analysis in videos. I did my Masters at <a href="https://www.iitg.ac.in/">Indian Institute of Technology Guwahati</a> under the supervision of <a href="https://www.iitg.ac.in/engfac/k.karthik/">Prof. Kannan Karthik</a> in 2012.
              </p>
	       <p>
                I like to play rhythm instruments in my free time. I also prefer to read books and occasionally do blogging. <a href="https://praveena2j.blogspot.com/">Here</a> is the collection of my musings. 
              </p>
		<p>
		   <font color="red"><strong>I am actively looking for research opportunities in both industry and academia</strong></font> 
	      </p>
              <p style="text-align:center">
                <a href="mailto:praveenrgp1988@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/GnanaPraveen_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=hOWAkqkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/GnanaPraveenR">Twitter</a> &nbsp/&nbsp
		<a href="https://www.researchgate.net/profile/Gnana-Rajasekhar">ResearchGate</a> &nbsp/&nbsp
                <a href="https://github.com/praveena2j">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/gnanapraveen/"> LinkedIn </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/praveen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/AJ_06303.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody>
		</tr> 

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Affiliations</heading>
        </td>
      </tr>
      </tbody></table>
    <table align="center">
        <tbody>
        <tr>
            <td width="22%" align="center">
                <a href="https://www.iitg.ac.in/" target="_blank">
                <img style="width:120px"  src="images/iit_logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://iisc.ac.in/" target="_blank">
                <img style="width:120px"  src="images/IIsc_logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://research.samsung.com/sri-b" target="_blank">
                <img style="width:120px" src="images/samsung.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://www.etsmtl.ca/en" target="_blank">
                <img style="width:120px" src="images/etslogo.png"></a>&nbsp &nbsp
            </td>
            <td width="22%" align="center">
                <a href="https://www.crim.ca/en/" target="_blank">
                <img style="width:120px" src="images/crim.png"></a>&nbsp &nbsp
            </td>
        </tr>
        <tr>
            <td width="22%" align="center"><font size="3">IITG<br>2010-2013</font></td>
            <td width="22%" align="center"><font size="3">IISc<br>2013</font></td>
            <td width="22%" align="center"><font size="3">Samsung Research<br>2014-2015</font></td>
	    <td width="22%" align="center"><font size="3">ETS<br>2018-2023</font></td>
            <td width="22%" align="center"><font size="3">CRIM<br>2023-present</font></td>
        </tr>
        </tbody>
    </table>
	<br>

	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		    
		 <heading>News</heading>
		    <ul style=“list-style-type:square”>
		<li><sup>
              </sup>Apr 2024: Our paper titled "United we stand, Divided we fall: Handling Weak Complementarity for Audio-Visual Emotion Recognition in Valence-Arousal Space" has been accepted at <a href="https://affective-behavior-analysis-in-the-wild.github.io/8th/">ABAW@CVPR 2025</a> Workshop </li>
		<li><sup>
		<font color="red"><strong>New!!</strong></font>
		</sup>Mar 2025: Achieved 2nd place in the valence-arousal challenge of 8th <a href="https://affective-behavior-analysis-in-the-wild.github.io/8th/">ABAW@CVPR 2025</a> competition </li>
		<li><sup>
		</sup>Feb 2025: Got selected for <a href="https://smash.ung.si/">SMASH</a> postdoctoral fellowship cofunded by MSCA</li>			    
		<li><sup>
              </sup>Dec 2024: LAVViT got accepted to <a href="https://2025.ieeeicassp.org/">ICASSP 2025</a> </li>
		<li><sup>
              </sup>Oct 2024: Received <a href="data/AIDH_Symp_PosterAwards_Oct2024_GPR.pdf">Best Poster award </a> for our work on "Dynamic Cross Attention for Emotion Recognition" at the <a href="https://aihealthsymposium2024.squarespace.com/">AI and Digital Health Symposium 2024</a>, Montreal, Canada </li>
		<li><sup>
              </sup>Oct 2024: Our work "Less is Enough: Adapting Pre-trained Vision Transformers for Audio-Visual Speaker Verification" has been accepted at <a href="https://neurips2024-enlsp.github.io/">ENLSP@NeurIPS 2024</a> Workshop </li>
		<li><sup>
		</sup>Jun 2024: Our work "Incongruity-Aware Cross-Modal Attention" has been accepted at <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4200690">IEEE Journal of Selected Topics in Signal Processing</a> [IF:8.7] </li>
		<li>May  2024: Serving as a reviewer for <b>ACM'MM 2024</b></li>
		<li>Apr  2024: Serving as a reviewer for <b>ECCV 2024</b></li>
		<li><sup>
              </sup>Apr 2024: Our work on "Recursive Joint Cross-modal attention" has been accepted at <a href="https://affective-behavior-analysis-in-the-wild.github.io/6th/">ABAW@CVPR 2024</a> Workshop </li>
		<li><sup>
              </sup>Mar 2024: Achieved 2nd place in the valence-arousal challenge of 6th <a href="https://affective-behavior-analysis-in-the-wild.github.io/6th/">ABAW@CVPR 2024</a> competition </li>
		<li><sup>
              </sup>Mar 2024: Our research article on joint cross-attention for audio-visual fusion has been featured in March issue of <a href="data/IEEEBiometricsNewsletter.pdf">IEEE Biometrics Newsletter</a>.</li>
		<li><sup>
              </sup>Mar 2024: One paper accepted at <a href="https://2024.ieeeicme.org/">ICME 2024</a> (CORE A)!</li>
		<li><sup>
              </sup>Mar 2024: Two papers accepted at <a href="https://fg2024.ieee-biometrics.org/">FG 2024</a>!</li>
		<li>Oct  2023: Our work on "RJCA for Speaker Verification" has been accepted at <a href="https://neurips.cc/Conferences/2023/">NeurIPS 2023</a> 3rd workshop on <a href="https://neurips2023-enlsp.github.io/">ENLSP</a></li>
		<li>Sep  2023: Serving as a reviewer for <b>WACV 2024</b></li>
		<li>Jun 2023: Serving as a reviewer for <b>ACM MM 2023</b></li>
		<li>Jun 2023: Presented our work "Recurrent Joint Attention for Audio-Visual Fusion in Regression-based Emotion Recognition" at <a href="https://2023.ieeeicassp.org/">ICASSP 2023</a></li>
		<li>May  2023: Successfully defended my Ph.D. Thesis titled "Deep Regression Models for Spatiotemporal Expression Recognition in Videos"</li>
		<li>Mar  2023: Started working as a post-doctoral researcher at Computer Research Institute of Montreal (<a href="https://www.crim.ca/en/">CRIM</a>)"</li>
		</ul>
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, affective computing, deep learning, and multimodal video understanding models. Most of my research revolves around video analytics, weakly supervised learning, facial behavior analysis, and multimodal (audio-visual) learning. I have published more than 20 papers at leading conferences and journals in machine learning and computer vision, including ICASSP, ICIP, ICME, FG, BMVC, CVPR, NeurIPS, TBIOM, and JSTSP with more than 400 citations on <a href="https://scholar.google.co.in/citations?user=hOWAkqkAAAAJ&hl=en">Google Scholar</a>. Selected publications of my work can be found below. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>


</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/LAVViT.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>LAVViT: Latent Audio-Visual Vision Transformers for Speaker Verification</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
             <br>
              <em><font color="green"><strong>ICASSP2025</strong></font></em> &nbsp 
		<br>
	       <em><font color="red"><strong>NeurIPS2024 Workshop on Efficient Natural Language and Speech Processing </strong></font></em>
              <br>
		<a href="">Code</a>
              <p></p>
              <p>
		In this work, we explored the prospect of adapting large pretrained Vision transformers for audio-visual speaker verification in a parameter efficient-manner with low computational complexity.
              </p>
            </td>
          </tr>	
		    
		    

		    
</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/DemoIACA.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10584250">
                <papertitle>Incongruity-Aware Cross-Modal Attention for Audio-Visual Fusion in Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em><font color="green"><strong>IEEE Journal of Special Topics in Signal Processing (JSTSP), 2024 </strong></font></em> &nbsp <font color="red"><strong>[Impact Factor: 8.7]</strong></font> 
              <br>
              <p></p>
              <p>
		In this work, we addressed the limitations of cross-atention in handling weak complementary relationships and proposed a novel framework of Incongruity aware cross-attention for effective fusion of audio and visual modalities for dimensional emotion recognition 
              </p>
            </td>
          </tr>	

</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/RJCMA.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024W/ABAW/html/Praveen_Recursive_Joint_Cross-Modal_Attention_for_Multimodal_Fusion_in_Dimensional_Emotion_CVPRW_2024_paper.html">
		      
                <papertitle>Recursive Joint Cross-Modal Attention for Multimodal Fusion in Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em><font color="green"><strong>CVPR2024 Workshop on Affective Behaviour Analysis in-the-Wild </strong></font></em> &nbsp <font color="red"><strong>Second Place in Valence-Arousal Challenge@CVPR2024</strong></font>
              <br>
              <p></p>
              <p>
				In this work, we introduced recursive fusion of joint cross-attention across audio, visual and text modalities for multimodal dimensional emotion recognition. We participated in the valence-arousal challenge of 6th ABAW competition and achieved second place. 
              </p>
            </td>
          </tr>	



		    

</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/DemoCR.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10687371">
                <papertitle>Cross-Attention is not always needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em><font color="green"><strong>ICME2024</strong></font></em> &nbsp <font color="red"><strong>(Oral)</strong></font>
              <br>
	<a href="https://github.com/praveena2j/Dynamic-CrossAttention">Code</a>
              <p></p>
              <p>
				In this work, we addressed the problem of weak complementary relationships across audio and visual modalities due to sarcastic or conflicting emotions. We address the limitations of cross-attention in handling weak complementary relationships by introducing a novel framework of Dynamic Cross-Attention. 
              </p>
            </td>
          </tr>	


		    
 </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/DCA_v4.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10581998">
                <papertitle>Dynamic Cross Attention for Audio-Visual Person Verification</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
              <br>
              <em><font color="green"><strong>FG2024</strong></font></em> &nbsp 
              <br>
              <p></p>
              <p>
				In this work, we addressed the problem of weak complementary relationships for effective audio-visual fusion for person verification using dynamic cross-attention. 
              </p>
            </td>
          </tr>	


		    
	 </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ENLSP.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10582018">
                <papertitle>Audio-Visual Person Verification based on Recursive Fusion of Joint Cross-Attention</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Jahangir Alam
		<br>
              <em><font color="green"><strong>FG2024</strong></font></em> </strong></font> <font color="red"><strong>(Selected as one of the best reviewed papers)</strong></font>
		<br>
	       <em><font color="red"><strong>NeurIPS2023 Workshop on Efficient Natural Language and Speech Processing </strong></font></em>
              <br>
		<a href="https://github.com/praveena2j/RJCAforSpeakerVerification">Code</a>
              <p></p>
              <p>
				Proposed a recursive joint cross-attention model for effective audio-visual fusion for person verification using recursive attention across audio and visual modalities in the videos. 
              </p>
            </td>
          </tr>	

		    
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ICASSP2023.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10095234">
                <papertitle>Recursive Joint Attention for Audio-Visual Fusion in Regression-based Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Patrick Cardinal, Eric Granger
              <br>
              <em><font color="green"><strong>ICASSP2023</strong></font></em> &nbsp <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/RecurrentJointAttentionwithLSTMs">Code</a>
              <p></p>
              <p>
				Proposed a recursive joint cross-attention model for effective fusion of audio and visual modalities by focusing on leveraging the intra-modal relationships using LSTMs and inter-modal relationships using recursive attention across audio and visual modalities in the video. 
              </p>
            </td>
          </tr>	
		
	  <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/TBIOM2022.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10005783">
                <papertitle>Audio-Visual Fusion for Emotion Recognition in the Valence-Arousal Space Using Joint Cross-Attention</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Patrick Cardinal, Eric Granger
              <br>
              <em><font color="green"><strong>IEEE Tran. on Biometrics, Behavior, and Identity Science (TBIOM), 2024 </strong></font></em> <a href="https://ieeexplore.ieee.org/document/10210211"><font color="red"><strong>Best of FG2021</strong><br><a href="https://praveena2j.github.io/data/IEEE%20Biometric%20Council%20Newsletter,%20March%202024.pdf"></font><font color="red"><strong>Featured in March issue of IEEE Biometrics Newsletter</strong></font>
              <br>
              <a href="https://github.com/praveena2j/Joint-Cross-Attention-for-Audio-Visual-Fusion">Code</a>

              <p></p>
              <p>
				Investigated the prospect of leveraging both intra and inter-modal relationships using joint cross-attentional audio-visual fusion. The robustness of the proposed model is further validated for missing audio modality along with interpretability analysis.  
              </p>
            </td>
          </tr>				
	<tr bgcolor="#ffffd0" >
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/CVPRW2022.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9856650">
                <papertitle>A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Wheidima Carneiro de Melo, Nasib Ullah, Haseeb Aslam, Osama Zeeshan, Théo Denorme, Marco Pedersoli, Alessandro L. Koerich, Simon Bacon, Patrick Cardinal, Eric Granger
              <br>
              <em><font color="green"><strong>CVPR2022 Workshop on Affective Behaviour Analysis in-the-Wild </strong></font></em> <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/praveena2j/JointCrossAttention-for-AV-Fusion">Code</a> /  <a href="https://arxiv.org/abs/2203.14779">arXiv</a> / <a href="https://github.com/praveena2j/praveena2j.github.io/blob/main/data/CVPRW2022_JCA.pdf">Slides</a> 

              <p></p>
              <p>
				Proposed a joint cross-attention model for effective fusion of audio and visual modalities by focusing on leveraging the intra and inter-modal relationships across audio and visual modalities in the video. 
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/FG2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9667055">
                <papertitle>Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em><font color="green"><strong>FG2021</strong></font></em> &nbsp <font color="red"><strong>(Oral)</strong></font> <br><font color="red"><strong>Selected as one of the best reviewed papers</strong></font> 
              <br>
              <a href="https://github.com/praveena2j/Cross-Attentional-AV-Fusion">Code</a> /  <a href="https://arxiv.org/abs/2111.05222">arXiv</a> / <a href="https://www.youtube.com/watch?v=v-ykwKyWfPg&t=103s">Video Presentation</a> / <a href="https://github.com/praveena2j/praveena2j.github.io/blob/main/data/FG2021_CA.pdf">Slides</a> / <a href="https://github.com/praveena2j/praveena2j.github.io/blob/main/data/FG2021_Poster.pdf">Poster</a>

              <p></p>
              <p>
				Proposed a cross-attentional model to leverage the intermodal characteristics across audio and visual modalities for effective audio-visual fusion.
              </p>
            </td>
          </tr>	

  		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/BMVC2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1087.pdf">
                <papertitle>Holistic Guidance for Occluded Person Re-Identification</papertitle>
              </a>
              <br>
              Madhu Kiran, <strong>R Gnana Praveen</strong>, Le Thanh Nguyen-Meidine, Soufiane Belharbi, Louis-Antoine Blais-Morin, Eric Granger
              <br>
              <em><font color="green"><strong>BMVC2021</strong></font></em> &nbsp <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/madhukiranets/HolisitcGuidanceOccReID2">Code</a> /  <a href="https://arxiv.org/abs/2104.06524">arXiv</a> 

              <p></p>
              <p>
				Proposed a Holistic Guidance (HG) method that relies on holistic (or non-occluded) data and its distribution in dissimilarity space to train on occluded datasets without the need of any external source.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/IVU2021.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S026288562100072X">
                <papertitle>Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em><font color="green"><strong>Image and Vision Computing (IVC), 2021 </strong></font></em> &nbsp <br><font color="red"><strong>[Impact Factor: 4.7]</strong></font>
              <br>
              <a href="https://github.com/praveena2j/WSDAOR">Code</a> /  <a href="https://arxiv.org/abs/2008.06392">arXiv</a> 

              <p></p>
              <p>
				Proposed a deep learning model for weakly-supervised Domain Adaptation with ordinal regression using coarse sequence level labels of videos. In particular, we have enforced ordinal relationship in the proposed model using gaussian distribution.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/SurveyTAC.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.09858">
                <papertitle>Weakly Supervised Learning for Facial Behavior Analysis: A Review</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em>IEEE Tran. on Affective Computing (Submitted)</em>, 2021
              <br>
              <p></p>
              <p>
				In this paper, we have presented a comprehensive taxonomy of weakly supervised learning models for facial behavior analysis along with its challenges and potential research directions.  
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/FG2020.png' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9320216">
                <papertitle>Deep Weakly Supervised Domain Adaptation for Pain Localization in Videos</papertitle>
              </a>
              <br>
              <strong>R Gnana Praveen</strong>, Eric Granger, Patrick Cardinal
              <br>
              <em><font color="green"><strong>FG2020</strong></font></em> &nbsp 
              <br>
		 <a href="https://arxiv.org/abs/1910.08173">arXiv</a> / <a href="https://www.youtube.com/watch?v=JC4oQIMGgjc&t=56s">Video Presentation</a> / <a href="https://github.com/praveena2j/praveena2j.github.io/blob/main/data/FG2020_Kit.pdf">Slides</a>  
              <p></p>
              <p>
				In this paper, we have proposed a novel framework of weakly supervised domain adaptation (WSDA) with limited sequence-level labels for pain localization in videos.
              </p>
            </td>
          </tr>	

		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ICIP2014.jpg' width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7025470">
                <papertitle>Super-pixel based crowd flow segmentation in H.264 compressed videos</papertitle>
              </a>
              <br>
              Sovan Biswas, <strong>R Gnana Praveen</strong>, R. Venkatesh Babu
              <br>
              <em><font color="green"><strong>ICIP2014</strong></font></em> &nbsp 
              <br>
              <p></p>
              <p>
				In this paper, we have proposed a simple yet robust novel approach for the segmentation of high-density crowd flows based on super-pixels in H.264 compressed videos.
              </p>
            </td>
          </tr>	


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
<div style="text-align: center;">
    <div style="display: inline-block; margin-top: 40px;">
        <table class="analytics">
            <tr>
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7dYDY7kWpvzwkrCWFCR3UOk_Z86oCnbQE5N2dADb_pM&cl=ffffff&w=a"></script>
            </tr>
        </table>
    </div>
</div>




	
</body>

</html>
